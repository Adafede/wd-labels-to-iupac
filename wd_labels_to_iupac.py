#!/usr/bin/env python3
"""
Wikidata-Labels-to-IUPAC-Converter (WLIC)

A comprehensive tool for converting labels from Wikidata to IUPAC names using OPSIN (Open Parser for Systematic IUPAC Nomenclature).

This script:
1. Queries Wikidata for chemical compounds with both labels and InChIKey
2. Processes labels through OPSIN to generate SMILES structures
3. Validates the generated structures by comparing InChIKeys
4. Exports results in multiple formats for further analysis

Author: Adriano Rutz
License: MIT
Version: 0.0.1
Python Requirements: >=3.10
"""

import os
import sys
import subprocess
import logging
import argparse
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Set
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import time
from urllib.parse import urlparse

import requests
import csv
from rdkit import Chem
from rdkit.Chem import inchi
from SPARQLWrapper import SPARQLWrapper, JSON
from tqdm import tqdm

# Version and metadata
__version__ = "0.0.1"
__author__ = "Adriano Rutz"
__email__ = "adafede@gmail.com"
__license__ = "MIT (code) and CC0 (data)"
__description__ = "Wikidata-Labels-to-IUPAC-Converter (WLIC)"


@dataclass
class Config:
    """
    Configuration settings for the WLIC converter.

    Attributes:
        opsin_url: URL to download OPSIN JAR file
        opsin_jar: Local filename for OPSIN JAR
        sparql_endpoint: Wikidata SPARQL endpoint URL
        max_workers: Maximum number of parallel processing threads
        timeout: Timeout for external operations (seconds)
        retry_attempts: Number of retry attempts for failed operations
        output_dir: Directory for output files
        chunk_size: Size of processing chunks for large datasets
    """

    opsin_url: str = "https://github.com/dan2097/opsin/releases/download/2.8.0/opsin-cli-2.8.0-jar-with-dependencies.jar"
    opsin_jar: str = "opsin-cli-2.8.0-jar-with-dependencies.jar"
    sparql_endpoint: str = "https://qlever.cs.uni-freiburg.de/api/wikidata"
    max_workers: int = 4
    timeout: int = 300
    retry_attempts: int = 3
    output_dir: str = "output"
    chunk_size: int = 1000
    export_sdf: bool = False


@dataclass
class ChemicalMatch:
    """
    Data structure representing a successful chemical name-to-structure match.

    Attributes:
        label: Chemical name as retrieved from Wikidata
        expected_inchikey: InChIKey from Wikidata
        actual_inchikey: InChIKey generated from OPSIN SMILES
        smiles: SMILES string generated by OPSIN
        confidence: Confidence score (1.0 for exact InChIKey match)
        source: Data source identifier
        processing_time: Time taken to process this compound (seconds)
    """

    label: str
    expected_inchikey: str
    actual_inchikey: str
    smiles: str
    confidence: float = 1.0
    source: str = "wikidata"
    processing_time: float = 0.0


class WLICConverterError(Exception):
    """Custom exception for WLIC converter operations."""

    pass


class Logger:
    """
    Enhanced logging utility with console and file output.

    Provides structured logging with different levels and automatic
    log file creation for debugging and audit purposes.
    """

    def __init__(self, name: str = "WLICConverter", level: int = logging.INFO):
        """
        Initialize logger with console and file handlers.

        Args:
            name: Logger name
            level: Logging level (DEBUG, INFO, WARNING, ERROR)
        """
        self.logger = logging.getLogger(name)
        self.logger.setLevel(level)

        # Prevent duplicate handlers
        if not self.logger.handlers:
            # Console handler for user feedback
            console_handler = logging.StreamHandler(sys.stdout)
            console_handler.setLevel(level)

            # File handler for detailed logs
            log_file = Path("WLIC_converter.log")
            file_handler = logging.FileHandler(log_file)
            file_handler.setLevel(logging.DEBUG)

            # Consistent formatting
            formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            console_handler.setFormatter(formatter)
            file_handler.setFormatter(formatter)

            self.logger.addHandler(console_handler)
            self.logger.addHandler(file_handler)

    def info(self, msg: str) -> None:
        """Log info message."""
        self.logger.info(msg)

    def error(self, msg: str) -> None:
        """Log error message."""
        self.logger.error(msg)

    def warning(self, msg: str) -> None:
        """Log warning message."""
        self.logger.warning(msg)

    def debug(self, msg: str) -> None:
        """Log debug message."""
        self.logger.debug(msg)


class DependencyManager:
    """
    Manages external dependencies required for the converter.

    Handles downloading and verification of OPSIN JAR file and
    Java runtime environment.
    """

    def __init__(self, config: Config, logger: Logger):
        """
        Initialize dependency manager.

        Args:
            config: Configuration object
            logger: Logger instance
        """
        self.config = config
        self.logger = logger

    def download_opsin(self) -> bool:
        """
        Download OPSIN JAR file if not already present.

        OPSIN (Open Parser for Systematic IUPAC Nomenclature) is used
        to convert chemical names to SMILES structures.

        Returns:
            bool: True if download successful or file already exists
        """
        if os.path.exists(self.config.opsin_jar):
            self.logger.info(f"OPSIN JAR found: {self.config.opsin_jar}")
            return True

        try:
            self.logger.info(f"Downloading OPSIN from {self.config.opsin_url}")

            # Download with progress bar
            response = requests.get(
                self.config.opsin_url,
                stream=True,
                timeout=self.config.timeout,
                headers={"User-Agent": "WLIC-Converter/0.0.1"},
            )
            response.raise_for_status()

            total_size = int(response.headers.get("content-length", 0))

            with open(self.config.opsin_jar, "wb") as f:
                with tqdm(
                    total=total_size,
                    unit="B",
                    unit_scale=True,
                    desc="Downloading OPSIN",
                ) as pbar:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            pbar.update(len(chunk))

            self.logger.info("OPSIN download completed successfully")
            return True

        except requests.exceptions.RequestException as e:
            self.logger.error(f"Failed to download OPSIN: {e}")
            self.logger.error(
                "Please check your internet connection or download manually"
            )
            return False

    def verify_java(self) -> bool:
        """
        Verify Java Runtime Environment is available.

        OPSIN requires Java to run. This method checks if Java is
        installed and accessible from the command line.

        Returns:
            bool: True if Java is available
        """
        try:
            result = subprocess.run(
                ["java", "-version"], capture_output=True, text=True, timeout=10
            )

            if result.returncode == 0:
                # Extract Java version for logging
                version_info = (
                    result.stderr.split("\n")[0] if result.stderr else "Unknown"
                )
                self.logger.info(f"Java installation verified: {version_info}")
                return True
            else:
                self.logger.error("Java not found or not working properly")
                return False

        except (subprocess.TimeoutExpired, FileNotFoundError) as e:
            self.logger.error(f"Java verification failed: {e}")
            self.logger.error(
                "Please install Java Runtime Environment (JRE) 8 or higher"
            )
            return False


class WikidataQuerier:
    """
    Handles SPARQL queries to Wikidata for chemical compound data.

    Retrieves chemical compounds that have both English names and
    InChIKey identifiers from Wikidata's chemical knowledge base.
    """

    def __init__(self, config: Config, logger: Logger):
        """
        Initialize Wikidata querier.

        Args:
            config: Configuration object
            logger: Logger instance
        """
        self.config = config
        self.logger = logger
        self.sparql = SPARQLWrapper(config.sparql_endpoint)
        self.sparql.setReturnFormat(JSON)
        # Set user agent for responsible API usage
        self.sparql.addCustomHttpHeader("User-Agent", "WLIC-Converter/0.0.1")

    def query_chemical_data(self) -> Dict[str, str]:
        """
        Query Wikidata for chemical compounds with names and InChIKeys.

        Executes a SPARQL query to retrieve chemical compounds that have:
        - English language labels (rdfs:label)
        - InChIKey identifiers (wdt:P235)

        Returns:
            Dict[str, str]: Mapping of chemical names to InChIKeys

        Raises:
            WLICConverterError: If query fails after all retry attempts
        """
        query = """
        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
        PREFIX wdt: <http://www.wikidata.org/prop/direct/>
        
        SELECT DISTINCT ?item ?inchikey ?label WHERE { 
            ?item wdt:P235 ?inchikey .
            ?item rdfs:label ?label .
            FILTER(lang(?label) = "en")
        }
        ORDER BY ?label
        """

        for attempt in range(self.config.retry_attempts):
            try:
                self.logger.info(
                    f"Querying Wikidata (attempt {attempt + 1}/{self.config.retry_attempts})"
                )

                start_time = time.time()
                self.sparql.setQuery(query)
                results = self.sparql.query().convert()
                query_time = time.time() - start_time

                self.logger.info(
                    f"Wikidata query completed in {query_time:.2f} seconds"
                )
                return self._process_wikidata_results(results)

            except Exception as e:
                self.logger.warning(f"Wikidata query attempt {attempt + 1} failed: {e}")
                if attempt < self.config.retry_attempts - 1:
                    wait_time = 2**attempt  # Exponential backoff
                    self.logger.info(f"Waiting {wait_time} seconds before retry...")
                    time.sleep(wait_time)
                else:
                    raise WLICConverterError(
                        f"Failed to query Wikidata after {self.config.retry_attempts} attempts: {e}"
                    )

    def _process_wikidata_results(self, results: dict) -> Dict[str, str]:
        """
        Process and validate Wikidata query results.

        Args:
            results: Raw SPARQL query results

        Returns:
            Dict[str, str]: Cleaned mapping of labels to InChIKeys
        """
        label_dict = {}
        duplicates = set()
        invalid_inchikeys = 0

        self.logger.info("Processing Wikidata results...")

        for result in results["results"]["bindings"]:
            try:
                label = result["label"]["value"].lower().strip()
                inchikey = result["inchikey"]["value"].strip()

                # Validate InChIKey format
                if not self._validate_inchikey(inchikey):
                    invalid_inchikeys += 1
                    continue

                # Handle duplicate labels with different InChIKeys
                if label in label_dict:
                    if label_dict[label] != inchikey:
                        duplicates.add(label)
                        # Keep the first occurrence
                        continue

                label_dict[label] = inchikey

            except KeyError as e:
                self.logger.warning(f"Missing required field in result: {e}")
                continue

        # Log processing statistics
        if duplicates:
            self.logger.warning(
                f"Found {len(duplicates)} duplicate labels with different InChIKeys (kept first occurrence)"
            )

        if invalid_inchikeys > 0:
            self.logger.warning(
                f"Filtered out {invalid_inchikeys} compounds with invalid InChIKeys"
            )

        self.logger.info(f"Processed {len(label_dict)} unique chemical compounds")
        return label_dict

    @staticmethod
    def _validate_inchikey(inchikey: str) -> bool:
        """
        Validate InChIKey format according to IUPAC standards.

        InChIKey format: XXXXXXXXXXXXXX-XXXXXXXXXX-X
        - 14 characters, hyphen, 10 characters, hyphen, 1 character
        - Total length: 27 characters

        Args:
            inchikey: InChIKey string to validate

        Returns:
            bool: True if format is valid
        """
        if not inchikey or len(inchikey) != 27:
            return False

        parts = inchikey.split("-")
        return (
            len(parts) == 3
            and len(parts[0]) == 14
            and len(parts[1]) == 10
            and len(parts[2]) == 1
            and all(c.isalnum() for part in parts for c in part)
        )


class OpsinProcessor:
    """
    Handles chemical name processing through OPSIN.

    OPSIN (Open Parser for Systematic IUPAC Nomenclature) converts
    systematic chemical names to SMILES molecular representations.
    """

    def __init__(self, config: Config, logger: Logger):
        """
        Initialize OPSIN processor.

        Args:
            config: Configuration object
            logger: Logger instance
        """
        self.config = config
        self.logger = logger

    def process_labels(self, labels: List[str]) -> List[str]:
        """
        Process chemical labels through OPSIN to generate SMILES.

        Uses batch processing for efficiency - all labels are processed
        in a single OPSIN invocation rather than individual calls.

        Args:
            labels: List of chemical names to process

        Returns:
            List[str]: SMILES strings (empty string for failed conversions)
        """
        if not labels:
            self.logger.warning("No labels provided for OPSIN processing")
            return []

        input_file = Path("opsin_input.txt")
        output_file = Path("opsin_output.txt")

        try:
            # Write all labels to input file
            with open(input_file, "w", encoding="utf-8") as f:
                f.write("\n".join(labels))

            self.logger.info(
                f"Processing {len(labels)} chemical names through OPSIN..."
            )

            # Execute OPSIN batch processing
            start_time = time.time()
            result = subprocess.run(
                [
                    "java",
                    "-jar",
                    self.config.opsin_jar,
                    "-osmi",  # Output SMILES format
                    str(input_file),
                    str(output_file),
                ],
                check=True,
                capture_output=True,
                text=True,
                timeout=self.config.timeout,
            )

            processing_time = time.time() - start_time
            self.logger.info(
                f"OPSIN processing completed in {processing_time:.2f} seconds"
            )

            # Read SMILES output
            if not output_file.exists():
                self.logger.error("OPSIN did not create output file")
                return [""] * len(labels)

            with open(output_file, "r", encoding="utf-8") as f:
                smiles = [line.strip() for line in f]

            # Ensure output length matches input length
            if len(smiles) != len(labels):
                self.logger.warning(
                    f"Output length mismatch: {len(smiles)} SMILES vs {len(labels)} labels"
                )
                # Pad with empty strings if needed
                while len(smiles) < len(labels):
                    smiles.append("")

            # Log processing statistics
            successful = sum(1 for s in smiles if s)
            success_rate = (successful / len(labels)) * 100
            self.logger.info(
                f"OPSIN success rate: {success_rate:.1f}% ({successful}/{len(labels)})"
            )

            return smiles

        except subprocess.CalledProcessError as e:
            self.logger.error(
                f"OPSIN processing failed with return code {e.returncode}"
            )
            if e.stderr:
                self.logger.error(f"OPSIN error output: {e.stderr}")
            return [""] * len(labels)

        except subprocess.TimeoutExpired:
            self.logger.error(
                f"OPSIN processing timed out after {self.config.timeout} seconds"
            )
            self.logger.error("Consider increasing timeout or reducing dataset size")
            return [""] * len(labels)

        except Exception as e:
            self.logger.error(f"Unexpected error during OPSIN processing: {e}")
            return [""] * len(labels)

        finally:
            # Clean up temporary files
            for temp_file in [input_file, output_file]:
                if temp_file.exists():
                    try:
                        temp_file.unlink()
                    except OSError as e:
                        self.logger.warning(
                            f"Could not remove temporary file {temp_file}: {e}"
                        )


class MoleculeValidator:
    """
    Validates molecular structures and compares with reference data.

    Uses RDKit to validate SMILES structures and generate InChIKeys
    for comparison with Wikidata reference values.
    """

    def __init__(self, logger: Logger):
        """
        Initialize molecule validator.

        Args:
            logger: Logger instance
        """
        self.logger = logger

    def validate_and_compare(
        self, labels: List[str], expected_inchikeys: List[str], smiles_list: List[str]
    ) -> Tuple[List[ChemicalMatch], Dict[str, int]]:
        """
        Validate SMILES structures and compare InChIKeys with reference data.

        Args:
            labels: Chemical names
            expected_inchikeys: Reference InChIKeys from Wikidata
            smiles_list: SMILES strings from OPSIN

        Returns:
            Tuple containing:
                - List of successful matches
                - Dictionary of processing statistics
        """
        matches = []
        stats = {
            "total_processed": 0,
            "valid_smiles": 0,
            "inchikey_matches": 0,
            "invalid_smiles": 0,
            "conversion_errors": 0,
        }

        self.logger.info("Validating molecular structures...")

        # Process in parallel for efficiency
        with ThreadPoolExecutor(
            max_workers=self.config.max_workers if hasattr(self, "config") else 4
        ) as executor:
            futures = []

            for label, expected_key, smiles in zip(
                labels, expected_inchikeys, smiles_list
            ):
                future = executor.submit(
                    self._validate_single, label, expected_key, smiles
                )
                futures.append(future)

            # Process results with progress bar
            for future in tqdm(
                as_completed(futures), total=len(futures), desc="Validating structures"
            ):
                try:
                    result = future.result()

                    if result["match"]:
                        matches.append(result["match"])

                    # Accumulate statistics
                    for key, value in result["stats"].items():
                        stats[key] += value

                except Exception as e:
                    self.logger.error(f"Validation error: {e}")
                    stats["conversion_errors"] += 1

        # Log final statistics
        if stats["total_processed"] > 0:
            success_rate = (stats["inchikey_matches"] / stats["total_processed"]) * 100
            self.logger.info(f"Validation complete: {success_rate:.2f}% success rate")

        return matches, stats

    def _validate_single(self, label: str, expected_inchikey: str, smiles: str) -> dict:
        """
        Validate a single molecular structure.

        Args:
            label: Chemical name
            expected_inchikey: Reference InChIKey
            smiles: SMILES string to validate

        Returns:
            dict: Validation result with match data and statistics
        """
        start_time = time.time()
        result = {
            "match": None,
            "stats": {
                "total_processed": 1,
                "valid_smiles": 0,
                "inchikey_matches": 0,
                "invalid_smiles": 0,
                "conversion_errors": 0,
            },
        }

        if not smiles:
            result["stats"]["invalid_smiles"] = 1
            return result

        try:
            # Parse SMILES with RDKit
            mol = Chem.MolFromSmiles(smiles)
            if mol is None:
                result["stats"]["invalid_smiles"] = 1
                return result

            result["stats"]["valid_smiles"] = 1

            # Generate InChIKey for comparison
            actual_inchikey = inchi.MolToInchiKey(mol)
            processing_time = time.time() - start_time

            # Check if InChIKeys match
            if actual_inchikey == expected_inchikey:
                result["stats"]["inchikey_matches"] = 1
                result["match"] = ChemicalMatch(
                    label=label,
                    expected_inchikey=expected_inchikey,
                    actual_inchikey=actual_inchikey,
                    smiles=smiles,
                    processing_time=processing_time,
                )

        except Exception as e:
            result["stats"]["conversion_errors"] = 1
            self.logger.debug(f"Error validating {label}: {e}")

        return result


class ResultExporter:
    """
    Handles export of results in multiple formats for analysis and archival.

    Exports successful matches, processing statistics, and metadata
    in formats suitable for scientific publication and data sharing.
    """

    def __init__(self, config: Config, logger: Logger):
        """
        Initialize result exporter.

        Args:
            config: Configuration object
            logger: Logger instance
        """
        self.config = config
        self.logger = logger
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(exist_ok=True)

    def export_results(
        self,
        matches: List[ChemicalMatch],
        stats: Dict[str, int],
        original_data: Dict[str, str],
        export_sdf: bool,
    ) -> None:
        """
        Export all results and metadata.

        Args:
            matches: List of successful chemical matches
            stats: Processing statistics
            original_data: Original Wikidata label-InChIKey mapping
        """
        self.logger.info("Exporting results...")

        # Export successful matches in multiple formats
        self._export_csv(matches, "chemical_matches.csv")
        self._export_json(matches, "chemical_matches.json")
        if export_sdf:
            self._export_sdf(matches, "chemical_matches.sdf")

        # Export metadata and statistics
        self._export_statistics(stats, len(original_data))
        self._export_metadata()

        # Export reference data
        self._export_original_data(original_data)

        # Create README for the dataset
        self._create_readme(matches, stats, original_data)

        self.logger.info(f"Results exported to {self.output_dir}")

    def _export_csv(self, matches: List[ChemicalMatch], filename: str) -> None:
        """Export matches to CSV format for spreadsheet analysis."""
        filepath = self.output_dir / filename

        with open(filepath, "w", newline="", encoding="utf-8") as f:
            if matches:
                fieldnames = list(asdict(matches[0]).keys())
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()

                for match in matches:
                    writer.writerow(asdict(match))
            else:
                # Write headers even for empty results
                writer = csv.writer(f)
                writer.writerow(
                    [
                        "label",
                        "expected_inchikey",
                        "actual_inchikey",
                        "smiles",
                        "confidence",
                        "source",
                        "processing_time",
                    ]
                )

        self.logger.info(f"Exported {len(matches)} matches to {filename}")

    def _export_json(self, matches: List[ChemicalMatch], filename: str) -> None:
        """Export matches to JSON format with metadata."""
        filepath = self.output_dir / filename

        data = {
            "metadata": {
                "title": "Labels-to-Structure Matches",
                "description": "Successful conversions of labels to molecular structures",
                "version": __version__,
                "total_matches": len(matches),
                "export_timestamp": time.strftime(
                    "%Y-%m-%d %H:%M:%S UTC", time.gmtime()
                ),
                "schema": {
                    "label": "Label from Wikidata",
                    "expected_inchikey": "Reference InChIKey from Wikidata",
                    "actual_inchikey": "InChIKey generated from OPSIN SMILES",
                    "smiles": "SMILES structure from OPSIN",
                    "confidence": "Match confidence (1.0 for exact InChIKey match)",
                    "source": "Data source identifier",
                    "processing_time": "Processing time in seconds",
                },
            },
            "data": [asdict(match) for match in matches],
        }

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

        self.logger.info(f"Exported {len(matches)} matches to {filename}")

    def _export_sdf(self, matches: List[ChemicalMatch], filename: str) -> None:
        """Export matches to SDF format for chemical structure viewing."""
        filepath = self.output_dir / filename

        try:
            with open(filepath, "w", encoding="utf-8") as f:
                for i, match in enumerate(matches):
                    try:
                        mol = Chem.MolFromSmiles(match.smiles)
                        if mol is not None:
                            # Add properties
                            mol.SetProp("_Name", match.label)
                            mol.SetProp("InChIKey", match.actual_inchikey)
                            mol.SetProp("SMILES", match.smiles)
                            mol.SetProp("Source", match.source)

                            # Write molecule to SDF
                            sdf_block = Chem.MolToMolBlock(mol)
                            f.write(sdf_block)
                            f.write("$$$$\n")
                    except Exception as e:
                        self.logger.warning(
                            f"Could not write molecule {match.label} to SDF: {e}"
                        )

            self.logger.info(f"Exported molecular structures to {filename}")
        except Exception as e:
            self.logger.warning(f"Could not create SDF file: {e}")

    def _export_statistics(self, stats: Dict[str, int], total_input: int) -> None:
        """Export detailed processing statistics."""
        filepath = self.output_dir / "processing_statistics.json"

        success_rate = (
            (stats["inchikey_matches"] / stats["total_processed"]) * 100
            if stats["total_processed"] > 0
            else 0
        )

        detailed_stats = {
            "processing_summary": {
                "total_input_compounds": total_input,
                "compounds_processed": stats["total_processed"],
                "successful_matches": stats["inchikey_matches"],
                "success_rate_percent": round(success_rate, 2),
            },
            "detailed_statistics": {
                **stats,
                "processing_timestamp": time.strftime(
                    "%Y-%m-%d %H:%M:%S UTC", time.gmtime()
                ),
                "opsin_success_rate": round(
                    (stats["valid_smiles"] / stats["total_processed"]) * 100, 2
                )
                if stats["total_processed"] > 0
                else 0,
            },
            "quality_metrics": {
                "structure_validation_rate": round(
                    (stats["valid_smiles"] / stats["total_processed"]) * 100, 2
                )
                if stats["total_processed"] > 0
                else 0,
                "inchikey_match_rate": round(success_rate, 2),
            },
        }

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(detailed_stats, f, indent=2)

    def _export_metadata(self) -> None:
        """Export dataset metadata for Zenodo compliance."""
        filepath = self.output_dir / "metadata.json"

        metadata = {
            "title": "Wikidata-Labels-to-IUPAC Conversion Dataset",
            "description": (
                "This dataset contains the results of converting labels from Wikidata "
                "to IUPAC name using OPSIN (Open Parser for Systematic IUPAC Nomenclature). "
                "It includes successful matches with validated InChIKey comparisons."
            ),
            "version": __version__,
            "creators": [
                {
                    "name": __author__,
                    "affiliation": "ETH Zurich",
                    "orcid": "0000-0003-0443-9902",
                }
            ],
            "license": __license__,
            "keywords": [
                "cheminformatics",
                "chemistry",
                "IUPAC",
                "OPSIN",
                "rdkit",
                "wikidata",
            ],
            "related_identifiers": [
                {
                    "identifier": "https://github.com/dan2097/opsin",
                    "relation": "requires",
                    "resource_type": "software",
                    "scheme": "url",
                },
                {
                    "identifier": "https://github.com/rdkit/rdkit",
                    "relation": "requires",
                    "resource_type": "software",
                    "scheme": "url",
                },
                {
                    "identifier": "https://www.wikidata.org",
                    "relation": "isSourceOf",
                    "resource_type": "dataset",
                    "scheme": "url",
                },
                {
                    "identifier": "10.59350/dycsw-qeq51",
                    "relation": "cites",
                    "resource_type": "other",
                    "scheme": "doi",
                },
            ],
            "methodology": {
                "data_source": "Wikidata SPARQL endpoint",
                "processing_tool": "OPSIN v2.8.0",
                "validation_method": "InChIKey comparison using RDKit",
                "programming_language": "Python 3.8+",
                "dependencies": ["RDKit", "SPARQLWrapper", "requests", "tqdm"],
            },
            "creation_date": time.strftime("%Y-%m-%d", time.gmtime()),
            "file_formats": ["CSV", "JSON", "SDF"],
            "quality_assurance": {
                "validation_performed": True,
                "validation_method": "InChIKey comparison",
                "error_handling": "Comprehensive logging and retry mechanisms",
            },
        }

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=2)

    def _export_original_data(self, data: Dict[str, str]) -> None:
        """Export original Wikidata reference data."""
        filepath = self.output_dir / "wikidata_reference.csv"

        with open(filepath, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["chemical_name", "inchikey", "source"])
            for label, inchikey in data.items():
                writer.writerow([label, inchikey, "wikidata"])

        self.logger.info(
            f"Exported {len(data)} reference compounds to wikidata_reference.csv"
        )

    def _create_readme(
        self,
        matches: List[ChemicalMatch],
        stats: Dict[str, int],
        original_data: Dict[str, str],
    ) -> None:
        """Create comprehensive README file for the dataset."""
        filepath = self.output_dir / "README.md"

        success_rate = (
            (stats["inchikey_matches"] / stats["total_processed"]) * 100
            if stats["total_processed"] > 0
            else 0
        )

        readme_content = f"""# Wikidata-Labels-to-IUPAC Conversion Dataset

## Overview

This dataset contains the results of converting labels from Wikidata to molecular structures using OPSIN (Open Parser for Systematic IUPAC Nomenclature). The conversion process validates results by comparing generated InChIKeys with values from Wikidata.

## Dataset Statistics

- **Total input compounds**: {len(original_data):,}
- **Successfully processed**: {stats['total_processed']:,}
- **Successful matches**: {stats['inchikey_matches']:,}
- **Success rate**: {success_rate:.2f}%
- **Processing date**: {time.strftime('%Y-%m-%d', time.gmtime())}

## Files Description

### Primary Data Files

- **`chemical_matches.csv`**: Successful matches in CSV format
  - Contains: chemical names, InChIKeys, SMILES structures, processing metadata
  - Suitable for: Spreadsheet analysis, statistical processing

- **`chemical_matches.json`**: Successful matches in JSON format
  - Contains: Same data as CSV with additional metadata and schema
  - Suitable for: Programmatic access, web applications

- **`chemical_matches.sdf`** (optional): Molecular structures in SDF format
  - Contains: 3D molecular structures with properties
  - Suitable for: Chemical visualization, molecular modeling

### Reference and Metadata Files

- **`wikidata_reference.csv`**: Original Wikidata compounds
  - Contains: All chemical names and InChIKeys from Wikidata query
  - Purpose: Complete reference dataset for reproducibility

- **`processing_statistics.json`**: Detailed processing statistics
  - Contains: Success rates, error counts, quality metrics
  - Purpose: Quality assessment and method validation

- **`metadata.json`**: Dataset metadata
  - Contains: Complete dataset description, methodology, provenance
  - Purpose: Zenodo compliance and data citation

## Methodology

### Data Source
Chemical compound data was retrieved from Wikidata using SPARQL queries targeting:
- Compounds with English language labels (`rdfs:label`)
- Compounds with InChIKey identifiers (`wdt:P235`)

### Processing Pipeline
1. **Data Retrieval**: SPARQL query to Wikidata endpoint
2. **Name Processing**: Batch conversion using OPSIN v2.8.0
3. **Structure Validation**: SMILES validation using RDKit
4. **Quality Control**: InChIKey comparison for accuracy verification
5. **Result Export**: Multi-format output generation

### Quality Assurance
- InChIKey format validation
- SMILES structure validation using RDKit
- Exact InChIKey matching for success determination
- Comprehensive error logging and statistics

## Software Requirements

### Runtime Dependencies
- Python 3.10+
- Java Runtime Environment (JRE) 8+
- OPSIN v2.8.0 (automatically downloaded)

### Python Packages
- RDKit (chemistry toolkit)
- SPARQLWrapper (SPARQL queries)
- requests (HTTP requests)
- tqdm (progress bars)

## Limitations and Considerations

1. **Coverage**: Results represent only compounds with both Wikidata names and InChIKeys
2. **Accuracy**: Success determined by exact InChIKey matching
3. **Scope**: Limited to systematic chemical nomenclature parseable by OPSIN
4. **Language**: Only English language chemical names processed

## Use

```bash
docker build -t wd-labels-to-iupac .

# Run (assuming script outputs to current directory)
docker run -v $(pwd):/app/output wd-labels-to-iupac
```

## Reproducibility

To reproduce this dataset:
1. Install required dependencies
2. Run the conversion script with default settings
3. Compare results with provided reference data

## Citation

If you use this dataset in your research, please cite:

```
Adriano Rutz ({time.strftime('%Y', time.gmtime())}). Wikidata-Labels-to-IUPAC Conversion Dataset. 
Version {__version__}. [Dataset]. Zenodo. https://doi.org/[DOI]
```

## License

This dataset is released under the {__license__} License. The original chemical data is from Wikidata (CC0 1.0 Universal).

## Contact

For questions or issues regarding this dataset:
- Email: {__email__}
- GitHub: https://github.com/Adafede/wd-labels-to-iupac

## Acknowledgments

- Egon Willighagen (0000-0001-7542-0286) for the original idea (see https://doi.org/10.59350/dycsw-qeq51)
- Wikidata contributors for chemical compound data
- OPSIN developers for the nomenclature parsing tool
- RDKit developers for chemical informatics capabilities
"""

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(readme_content)

        self.logger.info("Created README")


class WLICConverter:
    """
    Main application class orchestrating the complete conversion pipeline.

    Coordinates all components to perform the full workflow from data
    retrieval to result export, with comprehensive error handling and logging.
    """

    def __init__(self, config: Config = None):
        """
        Initialize the WLIC converter with configuration.

        Args:
            config: Configuration object (uses defaults if None)
        """
        self.config = config or Config()
        self.logger = Logger()

        # Initialize all pipeline components
        self.dependency_manager = DependencyManager(self.config, self.logger)
        self.wikidata_querier = WikidataQuerier(self.config, self.logger)
        self.opsin_processor = OpsinProcessor(self.config, self.logger)
        self.molecule_validator = MoleculeValidator(self.logger)
        self.result_exporter = ResultExporter(self.config, self.logger)

    def run(self) -> bool:
        """
        Execute the complete conversion pipeline.

        Returns:
            bool: True if pipeline completed successfully, False otherwise
        """
        try:
            self.logger.info("=" * 60)
            self.logger.info("Wikidata-Labels-to-IUPAC-Converter v" + __version__)
            self.logger.info("=" * 60)

            # Pipeline execution with detailed logging
            pipeline_start = time.time()

            # Step 1: Setup and validation
            if not self._setup_dependencies():
                return False

            # Step 2: Data retrieval
            self.logger.info("\n" + "=" * 40)
            self.logger.info("STEP 2: Retrieving chemical data from Wikidata")
            self.logger.info("=" * 40)

            label_inchikey_dict = self.wikidata_querier.query_chemical_data()
            if not label_inchikey_dict:
                self.logger.error("No chemical data retrieved from Wikidata")
                return False

            # Step 3: Structure generation
            self.logger.info("\n" + "=" * 40)
            self.logger.info("STEP 3: Generating molecular structures with OPSIN")
            self.logger.info("=" * 40)

            labels = list(label_inchikey_dict.keys())
            expected_inchikeys = list(label_inchikey_dict.values())
            smiles_results = self.opsin_processor.process_labels(labels)

            # Step 4: Validation and comparison
            self.logger.info("\n" + "=" * 40)
            self.logger.info("STEP 4: Validating structures and comparing InChIKeys")
            self.logger.info("=" * 40)

            matches, stats = self.molecule_validator.validate_and_compare(
                labels, expected_inchikeys, smiles_results
            )

            # Step 5: Results export
            self.logger.info("\n" + "=" * 40)
            self.logger.info("STEP 5: Exporting results and metadata")
            self.logger.info("=" * 40)

            self.result_exporter.export_results(
                matches,
                stats,
                label_inchikey_dict,
                self.config.export_sdf,
            )

            # Pipeline completion
            total_time = time.time() - pipeline_start
            self.logger.info("\n" + "=" * 60)
            self.logger.info("PIPELINE COMPLETED SUCCESSFULLY")
            self.logger.info(f"Total processing time: {total_time:.2f} seconds")
            self.logger.info(f"Successful matches: {len(matches)}")
            self.logger.info(f"Results exported to: {self.config.output_dir}")
            self.logger.info("=" * 60)

            return True

        except Exception as e:
            self.logger.error(f"Pipeline failed with error: {e}")
            self.logger.error("Check the log file for detailed error information")
            return False

    def _setup_dependencies(self) -> bool:
        """
        Setup and validate all required dependencies.

        Returns:
            bool: True if all dependencies are available
        """
        self.logger.info("\n" + "=" * 40)
        self.logger.info("STEP 1: Setting up dependencies")
        self.logger.info("=" * 40)

        # Verify Java installation
        if not self.dependency_manager.verify_java():
            self.logger.error("Java Runtime Environment is required but not found")
            self.logger.error(
                "Please install JRE 8 or higher and ensure 'java' is in PATH"
            )
            return False

        # Download OPSIN if needed
        if not self.dependency_manager.download_opsin():
            self.logger.error("Failed to obtain OPSIN JAR file")
            self.logger.error("Please check internet connection or download manually")
            return False

        self.logger.info("All dependencies verified successfully")
        return True


def create_sample_config() -> Config:
    """
    Create a sample configuration file for user customization.

    Returns:
        Config: Sample configuration object
    """
    return Config(
        max_workers=8,  # Increase for faster processing on multi-core systems
        timeout=600,  # Longer timeout for large datasets
        output_dir="wlic_conversion_results",
        retry_attempts=5,
        export_sdf=False,
    )


def main():
    """
    Main entry point with comprehensive command-line interface.

    Provides extensive configuration options and helpful usage information.
    """
    parser = argparse.ArgumentParser(
        description="Wikidata-Labels-to-IUPAC-Converter",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
Examples:
  {sys.argv[0]}                                    # Run with default settings
  {sys.argv[0]} --max-workers 8 --verbose         # Use 8 workers with verbose output
  {sys.argv[0]} --output-dir ./results --timeout 600  # Custom output and timeout
  {sys.argv[0]} --help                             # Show this help message

For more information, see the generated README file.

Version: {__version__}
Author: {__author__}
License: {__license__}
        """,
    )

    # Configuration options
    parser.add_argument(
        "--max-workers",
        type=int,
        default=4,
        metavar="N",
        help="Maximum number of parallel processing threads (default: 4)",
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default="output",
        metavar="DIR",
        help="Output directory for results (default: output)",
    )

    parser.add_argument(
        "--timeout",
        type=int,
        default=300,
        metavar="SECONDS",
        help="Timeout for external operations in seconds (default: 300)",
    )

    parser.add_argument(
        "--retry-attempts",
        type=int,
        default=3,
        metavar="N",
        help="Number of retry attempts for failed operations (default: 3)",
    )

    parser.add_argument(
        "--export-sdf",
        dest="export_sdf",
        action="store_true",
        help="Export SDF",
    )

    # Logging options
    parser.add_argument(
        "--verbose",
        "-v",
        action="store_true",
        help="Enable verbose logging (DEBUG level)",
    )

    parser.add_argument(
        "--quiet",
        "-q",
        action="store_true",
        help="Suppress non-error output (ERROR level only)",
    )

    # Utility options
    parser.add_argument(
        "--version",
        action="version",
        version=f"WLIC Converter {__version__}",
        help="Show version information and exit",
    )

    args = parser.parse_args()

    # Handle conflicting logging options
    if args.verbose and args.quiet:
        parser.error("--verbose and --quiet are mutually exclusive")

    # Create configuration from arguments
    config = Config(
        max_workers=args.max_workers,
        output_dir=args.output_dir,
        timeout=args.timeout,
        retry_attempts=args.retry_attempts,
        export_sdf=args.export_sdf,
    )

    # Set logging level based on user preference
    if args.verbose:
        log_level = logging.DEBUG
    elif args.quiet:
        log_level = logging.ERROR
    else:
        log_level = logging.INFO

    # Initialize and run converter
    try:
        # Update logger level
        logging.getLogger().setLevel(log_level)

        # Create and run converter
        converter = WLICConverter(config)
        success = converter.run()

        # Exit with appropriate code
        sys.exit(0 if success else 1)

    except KeyboardInterrupt:
        print("\nOperation cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"Unexpected error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
